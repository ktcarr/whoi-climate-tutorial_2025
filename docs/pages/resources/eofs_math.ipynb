{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3d3bf51-0cf9-4c56-b1fe-722132ab71aa",
   "metadata": {},
   "source": [
    "# Math of EOFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc3aa79-6254-47a3-9994-17a2e555f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## set plotting preferences and instantiate RNG\n",
    "sns.set(rc={\"axes.facecolor\": \"white\", \"axes.grid\": False})\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34c3a3b-5bff-4f13-92c7-7ffb364e45f9",
   "metadata": {},
   "source": [
    "## Generate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc6e229-dd26-4204-b056-af6fbabf0f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_cov(Cxx, n_samples):\n",
    "    \"\"\"Draw 'n_samples' from specified covariance matrix\"\"\"\n",
    "\n",
    "    ## check pos semi-definite\n",
    "    assert np.all(np.diag(Cxx) > 0)\n",
    "    assert np.allclose(Cxx.T, Cxx)\n",
    "\n",
    "    ## cholesky decompose\n",
    "    L = np.linalg.cholesky(Cxx)\n",
    "\n",
    "    ## draw IID samples\n",
    "    Y = rng.normal(size=(Cxx.shape[0], n_samples))\n",
    "\n",
    "    return L @ Y\n",
    "\n",
    "\n",
    "def plot_setup(ax):\n",
    "    \"\"\"modify plot to preferred style\"\"\"\n",
    "\n",
    "    ## equal aspect ratio\n",
    "    ax.set_aspect(\"equal\")\n",
    "\n",
    "    ## plot axes\n",
    "    kwargs = dict(c=\"k\", lw=0.5)\n",
    "    ax.axhline(0, **kwargs)\n",
    "    ax.axvline(0, **kwargs)\n",
    "\n",
    "    ## hide tick labels\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1dfed3-92c6-45d4-973a-710a5bdcebfc",
   "metadata": {},
   "source": [
    "## Plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31a5c00-962b-4544-aa47-54c50ed8ef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## specify covariance matrix\n",
    "Cxx = np.array([[1.3, -0.7], [-0.7, 0.8]])\n",
    "\n",
    "## draw some samples\n",
    "X = sample_from_cov(Cxx, n_samples=40)\n",
    "\n",
    "## plot samples\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax = plot_setup(ax)\n",
    "ax.scatter(X[0], X[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923a60c2-e3ea-40de-a507-5daaed624ec5",
   "metadata": {},
   "source": [
    "## Geometric representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9fe68c-e2c8-40c3-bb3d-934e39938a52",
   "metadata": {},
   "source": [
    "### Pick a sample and pattern to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f64e40-db38-4ded-a4cc-314dd5e76a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "## draw some samples\n",
    "x = np.array([2, 1])\n",
    "v = np.array([2, 2])\n",
    "\n",
    "## get projection of x on v\n",
    "xhat = (x.T @ v) / (v.T @ v) * v\n",
    "\n",
    "## get reconstruction error\n",
    "error = x - xhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747b9a6c-7e20-4e75-90cd-625c8f95e061",
   "metadata": {},
   "source": [
    "### Plot sample/pattern with error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01189f5-ebc1-484b-8185-1eeef25791c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## labels for plotting\n",
    "recon_label = r\"Reconstruction $\\left(\\hat{\\mathbf{x}}=\\frac{\\mathbf{p}^T\\mathbf{x}}{\\mathbf{p}^T\\mathbf{p}}\\mathbf{p}\\right)$\"\n",
    "error_label = r\"Error $\\left(\\mathbf{e}=\\mathbf{x}-\\hat{\\mathbf{x}}\\right)$\"\n",
    "\n",
    "## default args for plotting vector\n",
    "arrow_kwargs = dict(head_width=0.12, width=0.02, length_includes_head=True)\n",
    "\n",
    "## colors for plotting\n",
    "colors = sns.color_palette()\n",
    "\n",
    "## plot samples\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.scatter(0, 0, c=\"k\", s=50, zorder=10)\n",
    "\n",
    "for i, (u, label) in enumerate(\n",
    "    zip([x, v], [r\"Sample ($\\mathbf{x}$)\", r\"Pattern ($\\mathbf{p}$)\"])\n",
    "):\n",
    "\n",
    "    ## plot sample and pattern\n",
    "    arrow_kwargs = dict(head_width=0.12, width=0.015, length_includes_head=True)\n",
    "    ax.arrow(0, 0, dx=u[1], dy=u[0], color=colors[i], **arrow_kwargs, label=label)\n",
    "\n",
    "\n",
    "## plot projection\n",
    "ax.arrow(\n",
    "    0,\n",
    "    0,\n",
    "    dx=xhat[1],\n",
    "    dy=xhat[0],\n",
    "    color=\"k\",\n",
    "    ls=\"--\",\n",
    "    alpha=0.5,\n",
    "    **arrow_kwargs,\n",
    "    label=recon_label,\n",
    ")\n",
    "\n",
    "## plot error\n",
    "ax.arrow(\n",
    "    x=xhat[1],\n",
    "    y=xhat[0],\n",
    "    dx=error[1],\n",
    "    dy=error[0],\n",
    "    color=\"k\",\n",
    "    **arrow_kwargs,\n",
    "    label=error_label,\n",
    ")\n",
    "\n",
    "ax.legend(prop=dict(size=10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea19ffef-61e9-4d9c-8472-c4d0d2145237",
   "metadata": {},
   "source": [
    "## Reconstruction error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2002759-8c90-49d5-8f8d-6cb93ffe3e10",
   "metadata": {},
   "source": [
    "The squared length of the vector $\\mathbf{x}$ is $||\\mathbf{x}||_2^2=\\mathbf{x}^T\\mathbf{x}$, and the length of the reconstruction vector is given by:\n",
    "\\begin{align}\n",
    "    ||\\hat{\\mathbf{x}}||_2^2 &= \\hat{\\mathbf{x}}^T\\hat{\\mathbf{x}} = \\left(\\frac{\\mathbf{p}^T\\mathbf{x}}{\\mathbf{p}^T\\mathbf{p}}\\mathbf{p}\\right)^T\\left(\\frac{\\mathbf{p}^T\\mathbf{x}}{\\mathbf{p}^T\\mathbf{p}}\\mathbf{p}\\right)\\\\\n",
    "    &= \\frac{\\left(\\mathbf{p}^T\\mathbf{x}\\right)^2}{\\mathbf{p}^T\\mathbf{p}} \\\\\n",
    "    &= \\frac{\\mathbf{p}^T\\mathbf{x}\\mathbf{x}^T\\mathbf{p}}{\\mathbf{p}^T\\mathbf{p}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b73c2a1-e63d-47e0-965f-5efdee01f68e",
   "metadata": {},
   "source": [
    "Next, use the Pythagorean theorem to find the length of the error vector: \n",
    "\\begin{align}\n",
    "    \\left| \\left| \\hat{\\mathbf{x}} \\right| \\right|_2^2 + ||\\mathbf{e}||_2^2 &= ||\\mathbf{x}||_2^2\\\\\n",
    "    \\implies ||\\mathbf{e}||_2^2 &= ||\\mathbf{x}||_2^2 - \\left| \\left| \\hat{\\mathbf{x}} \\right| \\right|_2^2\\\\\n",
    "    &= \\mathbf{x}^T\\mathbf{x} - \\frac{\\mathbf{p}^T\\mathbf{x}\\mathbf{x}^T\\mathbf{p}}{\\mathbf{p}^T\\mathbf{p}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7302b9d1-6bdb-4297-9b3f-1ca48cb5dd1a",
   "metadata": {},
   "source": [
    "## Choosing the \"optimal\" $\\mathbf{p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e26184f-357a-4837-9928-994d5bea9d98",
   "metadata": {},
   "source": [
    "Let's define the \"optimal\" $\\mathbf{p}$ as the pattern which minimizes the reconstruction data for the entire dataset. The mean reconstruction error for the full dataset is given by:\n",
    "\\begin{align}\n",
    "    \\mathcal{R} &= \\frac{1}{n}\\sum^n_j \\mathbf{x}_j^T\\mathbf{x}_j - \\frac{\\mathbf{p}^T\\mathbf{x}_j\\mathbf{x}_j^T\\mathbf{p}}{\\mathbf{p}^T\\mathbf{p}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23de26bd-d403-4ba9-b275-825b5e0a6efd",
   "metadata": {},
   "source": [
    "Note that the choice of $\\mathbf{p}$ which minimizes $\\mathcal{R}\\propto \\sum_j - \\frac{\\mathbf{p}^T\\mathbf{x}_j\\mathbf{x}_j^T\\mathbf{p}}{\\mathbf{p}^T\\mathbf{p}}$ will *maximize* the squared projection length of the data onto the pattern, $\\mathcal{P}=\\frac{\\sum_j \\mathbf{p}^T\\mathbf{x}_j\\mathbf{x}_j^T\\mathbf{p}}{\\mathbf{p}^T\\mathbf{p}}$! Therefore, without loss of generality, let's write down the optimization problem as:\n",
    "\\begin{align}\n",
    "    \\mathbf{p}^* &= \\text{argmax}_\\mathbf{p} \\left(\\frac{\\sum_j \\mathbf{p}^T\\mathbf{x}_j\\mathbf{x}_j^T\\mathbf{p}}{\\mathbf{p}^T\\mathbf{p}}\\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165203f8-b20b-44f6-b82a-6a3419602bd5",
   "metadata": {},
   "source": [
    "To find $\\mathbf{p}^*$, start by rewriting the squared projection length:\n",
    "\\begin{align}\n",
    "    \\frac{\\sum_j \\mathbf{p}^T\\mathbf{x}_j\\mathbf{x}_j^T\\mathbf{p}}{\\mathbf{p}^T\\mathbf{p}} &= \\frac{\\mathbf{p}^T\\mathbf{C}_{xx}\\mathbf{p}}{\\mathbf{p}^T\\mathbf{p}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0d1a20-d804-4c63-92ff-1f2d0f54250d",
   "metadata": {},
   "source": [
    "Where we set $\\mathbf{C}_{xx}=\\sum_j\\mathbf{x}_j\\mathbf{x}_j^T$. Next, write $\\mathbf{C}_{xx}$ in terms of $\\mathbf{X}$'s singular value decomposition. Letting $\\mathbf{X}=\\mathbf{V}\\boldsymbol{\\Lambda}\\mathbf{U}^T$, we have $\\mathbf{C}_{xx}=\\mathbf{XX}^T=\\mathbf{V}\\boldsymbol{\\Lambda}^2\\mathbf{V}^T$, using the SVD property $\\mathbf{U}^T\\mathbf{U}=\\mathbf{I}$ (note that the columns of $\\mathbf{V}$ are eigenvectors of $\\mathbf{C}_{xx}$; to see this, right multiply both sides by $\\mathbf{V}$ and use the property $\\mathbf{V}^T\\mathbf{V}=\\mathbf{I}$).  \n",
    "\n",
    "Next, write $\\mathbf{p}$ in terms of $\\mathbf{V}$; that is $\\mathbf{p}=\\sum_i \\alpha_i\\mathbf{v}_i = \\mathbf{V}\\boldsymbol{\\alpha}$. Substituting, we have:\n",
    "\\begin{align}\n",
    "    \\frac{\\mathbf{p}^T\\mathbf{C}_{xx}\\mathbf{p}}{\\mathbf{p}^T\\mathbf{p}} &= \\frac{\\left(\\boldsymbol{\\alpha}^T \\mathbf{V}^T\\right)\\left(\\mathbf{V}\\boldsymbol{\\Lambda}^2\\mathbf{V}^T\\right)\\mathbf{V}\\boldsymbol{\\alpha} }{\\left(\\boldsymbol{\\alpha}^T \\mathbf{V}^T\\right)\\left(\\mathbf{V}\\boldsymbol{\\alpha}\\right)}\\\\\n",
    "    &= \\frac{\\boldsymbol{\\alpha^T\\Lambda^2\\alpha}}{\\boldsymbol{\\alpha}^T\\boldsymbol{\\alpha}}\\\\\n",
    "    &= \\frac{\\sum_j \\lambda_j^2\\alpha_j^2}{\\sum_j\\alpha_j^2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58f3387-7cd9-4620-8d8f-b11a96366e2a",
   "metadata": {},
   "source": [
    "Note the last quantity represents a weighted average of $\\mathbf{C}_{xx}$'s eigenvalues, with the weight of the $i^{th}$ eigenvalue given by $\\alpha_i^2$. To maximize the weighted average, set the weight for all non-leading eigenvalues to zero such that all the weight is on the largest eigenvalue. That is, set $\\boldsymbol{\\alpha}^T = \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\end{bmatrix}$. Then we have: $\\mathbf{p}=\\mathbf{V}\\boldsymbol{\\alpha}=\\mathbf{v}_1$; that is, $\\mathbf{p}$ is the leading eigenvector of $\\mathbf{C}_{xx}$ (note $||\\mathbf{v}_1||_2^2=1$ if obtained via SVD)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ab11c8-5692-4cff-9343-4e013a1f4cb4",
   "metadata": {},
   "source": [
    "## Explained variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cdff15-9d66-4951-a7b0-faee05e6ccaa",
   "metadata": {},
   "source": [
    "Recall we decomposed each sample into a reconstruction and an error, with the square of the sample length equal to the square of the reconstruction length plus the square of the error length:\n",
    "\\begin{align}\n",
    "    ||\\mathbf{x}_j||^2 &= ||\\hat{\\mathbf{x}}_j||^2 + ||\\mathbf{e}_j||^2\n",
    "\\end{align}\n",
    "Summing over all samples, we have:\n",
    "\\begin{align}\n",
    "    \\sum_j||\\mathbf{x}_j||^2 &= \\sum_j ||\\hat{\\mathbf{x}}_j||^2 + \\sum_j||\\mathbf{e}_j||^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d84758-9c78-4ca4-9d99-3b3ab1b8029a",
   "metadata": {},
   "source": [
    "The fraction of explained variance is the ratio of the length of the reconstruction term to the total term:\n",
    "\\begin{align}\n",
    "    r &= \\frac{\\sum_j ||\\hat{\\mathbf{x}}_j||^2}{\\sum_j ||\\mathbf{x}_j||^2}\\\\\n",
    "    &= \\frac{\\text{tr}\\left[\\left(\\mathbf{pp}^T\\mathbf{X}\\right)^T\\left(\\mathbf{pp}^T\\mathbf{X}\\right)\\right]}{\\text{tr}\\left(\\mathbf{X}^T\\mathbf{X}\\right)}\\\\\n",
    "    &= \\frac{\\text{tr}\\left[\\mathbf{X}^T\\mathbf{pp}^T \\mathbf{pp}^T\\mathbf{X} \\right]}{\\text{tr}\\left(\\mathbf{X}^T\\mathbf{X}\\right)} =  \\frac{\\text{tr}\\left[\\mathbf{X}^T\\mathbf{p} \\mathbf{p}^T\\mathbf{X} \\right]}{\\text{tr}\\left(\\mathbf{X}^T\\mathbf{X}\\right)}\\\\\n",
    "    &= \\frac{\\mathbf{p}^T\\mathbf{X} \\mathbf{X}^T\\mathbf{p}}{\\text{tr}\\left(\\mathbf{X}\\mathbf{X}^T\\right)}\n",
    "\\end{align}\n",
    "Where we used the commutative property of trace: $\\text{tr}\\left(\\mathbf{A}\\mathbf{B}^T\\right)=\\text{tr}\\left(\\mathbf{A}^T\\mathbf{B}\\right)$. Then, using the eigenvalue property of $\\mathbf{p}$, we have $\\mathbf{p}^T\\mathbf{X} \\mathbf{X}^T\\mathbf{p} = \\mathbf{p}^T\\left(\\lambda^2 \\mathbf{p}\\right)=\\lambda^2$. For the numerator, write $\\mathbf{XX}^T$ in terms of it's eigenvalue decomposition:\n",
    "\\begin{align}\n",
    "    \\text{tr}\\left(\\mathbf{XX}^T\\right) &= \\text{tr}\\left(\\sum_j\\lambda_j^2 \\mathbf{v}_j\\mathbf{v}_j^T\\right) = \\sum_j\\lambda_j^2\\cdot\\text{tr}\\left( \\mathbf{v}_j\\mathbf{v}_j^T\\right)= \\sum_j\\lambda_j^2\\cdot\\text{tr}\\left( \\mathbf{v}_j^T\\mathbf{v}_j\\right) = \\sum_j\\lambda_j^2\n",
    "\\end{align}\n",
    "Therefore, the ratio of variance explained by the $i^{th}$ pattern is:\n",
    "\\begin{align}\n",
    "    r &= \\frac{\\lambda_i^2}{\\sum_j\\lambda_j^2}\n",
    "\\end{align}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
