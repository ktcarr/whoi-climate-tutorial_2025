{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8de8c8b-a3c9-4277-8ffc-38a8a8700eb5",
   "metadata": {},
   "source": [
    "# Azores high: data prep\n",
    "In this notebook, we'll load global data from WHOI's CMIP server,  \"reduce\" it by trimming in time and space, and save the pre-processed data locally. While not always possible, saving a reduced version of the data can speed up subsequent analysis: the code will run much faster if we can fit the data into \"memory\" (i.e., random access memory, or RAM).\n",
    "\n",
    "To compute the Azores High index, download the trimmed data for reanalysis and CESM last millenium ensemble here: [https://drive.google.com/drive/folders/1kFF6a-ArVBdNaAK-3TKj-tpSV6Vx8yk4?usp=sharing](https://drive.google.com/drive/folders/1kFF6a-ArVBdNaAK-3TKj-tpSV6Vx8yk4?usp=sharing) (the folder called \"LME\" contains the trimmed last millenium ensemble data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b35aa8-7384-4906-9d77-f463ff0cebc2",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e13d2c-e10b-4b5b-8f75-7a70dc1be3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cmocean\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "## custom imports\n",
    "# import src.utils\n",
    "# import src.utils_azores\n",
    "\n",
    "## Set plotting defaults\n",
    "sns.set(rc={\"axes.facecolor\": \"white\", \"axes.grid\": False})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07517718-7cad-49ae-a03a-2e8d9d3f0910",
   "metadata": {},
   "source": [
    "## Define constants/functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3512b5-62f3-421f-84c3-565c0ed9c5fd",
   "metadata": {},
   "source": [
    "__Last millenium ensemble (LME) data access__: If you don't have access to the \"clidex\" data server, download the LME folder from the shared google drive, and save it to your local project's data filepath, ```DATA_FP```.  \n",
    "\n",
    "__Reanalysis data access__: Note that this script uses reanalysis data on the CMIP<b>5</b> server, rather than the CMIP<b>6</b> server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c8e7f4-423b-4377-a6b9-9d14220536c8",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b149b8f-cf0f-41a5-8ec8-1329747f8a37",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def switch_longitude_range(data):\n",
    "    \"\"\"move longitude for dataset or dataarray from [0,360)\n",
    "    to (-180, 180]. Function accepts xr.dataset/xr.dataarray\n",
    "    and returns object of same type\"\"\"\n",
    "\n",
    "    ## get updated longitude coordinate\n",
    "    updated_longitude = convert_longitude(data.lon.values)\n",
    "\n",
    "    ## sort updated coordinate to be increasing\n",
    "    updated_longitude = np.sort(updated_longitude)\n",
    "\n",
    "    ## sort data (\"reindex\") according to update coordinate\n",
    "    data = data.reindex({\"lon\": updated_longitude})\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def convert_longitude(longitude):\n",
    "    \"\"\"move longitude from range [0,360) to (-180,180].\n",
    "    Function accepts and returns a numpy array representing longitude values\"\"\"\n",
    "\n",
    "    ## find indices of longitudes which will become negative\n",
    "    is_neg = longitude > 180\n",
    "\n",
    "    ## change values at these indices\n",
    "    longitude[is_neg] = longitude[is_neg] - 360\n",
    "\n",
    "    return longitude\n",
    "\n",
    "\n",
    "def reverse_latitude(data):\n",
    "    \"\"\"Change direction of latitude from [-90,90] to [90,-90]\n",
    "    or vice versa\"\"\"\n",
    "\n",
    "    lat_reversed = data.lat.values[::-1]\n",
    "    data = data.reindex({\"lat\": lat_reversed})\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def djf_avg(data):\n",
    "    \"\"\"function to trim data to Dec/Jan/Feb (DJF) months\"\"\"\n",
    "\n",
    "    ## subset data\n",
    "    data_seasonal_avg = data.resample(time=\"QS-DEC\").mean()\n",
    "\n",
    "    ## get annual average for DJF\n",
    "    is_djf = data_seasonal_avg.time.dt.month == 12\n",
    "    data_djf_avg = data_seasonal_avg.sel(time=is_djf)\n",
    "\n",
    "    ## drop 1st and last averages, bc they only have\n",
    "    ## 2 samples and 1 sample, respectively\n",
    "    data_djf_avg = data_djf_avg.isel(time=slice(1, -1))\n",
    "\n",
    "    ## Replace time index with year, corresponding to january.\n",
    "    ## '+1' is because 'time's year uses december\n",
    "    year = data_djf_avg.time.dt.year + 1\n",
    "    data_djf_avg[\"time\"] = year\n",
    "    data_djf_avg = data_djf_avg.rename({\"time\": \"year\"})\n",
    "\n",
    "    return data_djf_avg\n",
    "\n",
    "\n",
    "def spatial_avg(data, lon_range=[None, None], lat_range=[None, None]):\n",
    "    \"\"\"get global average of a quantity over the sphere.\n",
    "    Data is xr.dataarray/xr.dataset with  a 'regular' lon/lat grid\n",
    "    (equal lon/lat spacing between all data points)\"\"\"\n",
    "\n",
    "    ## get latitude-dependent weights\n",
    "    # first, convert latitude from degrees to radians\n",
    "    # conversion factor = (2 pi radians)/(360 degrees)\n",
    "    latitude_radians = data.lat * (2 * np.pi) / 360\n",
    "    cos_lat = np.cos(latitude_radians)\n",
    "\n",
    "    ## Next, trim data to specified range\n",
    "    data_trim = data.sel(lon=slice(*lon_range), lat=slice(*lat_range))\n",
    "    cos_lat_trim = cos_lat.sel(lat=slice(*lat_range))\n",
    "\n",
    "    ## Next, compute weighted avg\n",
    "    data_weighted = data_trim.weighted(weights=cos_lat_trim)\n",
    "    data_avg = data_weighted.mean([\"lat\", \"lon\"])\n",
    "\n",
    "    return data_avg\n",
    "\n",
    "\n",
    "def spatial_int(data, lon_range=[None, None], lat_range=[None, None]):\n",
    "    \"\"\"compute spatial integral of a quantity on the sphere. For convenience,\n",
    "    assume regular grid (constant lat/lon)\"\"\"\n",
    "\n",
    "    ## Get latitude/longitude in radians.\n",
    "    ## denote (lon,lat) in radians as (theta, phi)\n",
    "    rad_per_deg = 2 * np.pi / 360\n",
    "    theta = data.lon * rad_per_deg\n",
    "    phi = data.lat * rad_per_deg\n",
    "\n",
    "    ## get differences for integration (assumes constant differences)\n",
    "    dtheta = theta[1] - theta[0]\n",
    "    dphi = phi[1] - phi[0]\n",
    "\n",
    "    ## broadcast to grid\n",
    "    dtheta = dtheta * xr.ones_like(theta)\n",
    "    dphi = dphi * xr.ones_like(phi)\n",
    "\n",
    "    ## Get area of patch\n",
    "    R = 6.371e6  # radius of earth in m\n",
    "    dA = R**2 * np.cos(phi) * dphi * dtheta\n",
    "\n",
    "    ## Integrate\n",
    "    data_int = (data * dA).sum([\"lat\", \"lon\"])\n",
    "\n",
    "    return data_int\n",
    "\n",
    "\n",
    "def get_trend(data, dim=\"year\"):\n",
    "    \"\"\"Get linear trend for an xr.dataarray along specified dimension\"\"\"\n",
    "\n",
    "    ## Get coefficients for best fit\n",
    "    polyfit_coefs = data.polyfit(dim=dim, deg=1)[\"polyfit_coefficients\"]\n",
    "\n",
    "    ## Get best fit line (linear trend in this case)\n",
    "    trend = xr.polyval(data[dim], polyfit_coefs)\n",
    "\n",
    "    return trend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce14af9-b06e-42f5-83e2-92cdfa2de59b",
   "metadata": {},
   "source": [
    "### Azores-specific utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40b2457-c303-4794-b26d-e06167f996c7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_AHA(slp, slp_global_avg, norm_type=\"global_mean\"):\n",
    "    \"\"\"compute Azores High Area index, similar to Cresswell-Clay et al. (2022).\n",
    "    Defined as: area of Azores region which has (normalized) SLP exceeding 0.5 std\n",
    "    of long term average. Normalized SLP is defined as local SLP minus\n",
    "    globally-averaged SLP.\n",
    "\n",
    "    Args:\n",
    "    - slp is gridded SLP data (at global scale)\n",
    "    - norm_type is one of {None, \"global_mean\", \"detrend\"} specifying\n",
    "        how to normalize the AHA index\n",
    "\n",
    "    Note: returns area in KM^2.\n",
    "    \"\"\"\n",
    "\n",
    "    ## get SLP anomaly in Azores regions\n",
    "    slp_azores = trim_to_azores(slp)\n",
    "\n",
    "    ## get normalized anomaly (func of lon/lat)\n",
    "    if norm_type == \"global_mean\":\n",
    "        slp_azores_norm = slp_azores - slp_global_avg\n",
    "\n",
    "    elif norm_type == \"detrend\":\n",
    "        trend = get_trend(slp_global_avg)\n",
    "        slp_azores_norm = slp_azores - trend\n",
    "\n",
    "    elif norm_type is None:\n",
    "        slp_azores_norm = slp_azores\n",
    "\n",
    "    else:\n",
    "        print(\"Error: specify valid normalization type.\")\n",
    "\n",
    "    ## Get standard deviation (func of lon/lat)\n",
    "    slp_azores_mean = slp_azores_norm.mean(\"year\")\n",
    "    slp_azores_std = slp_azores_norm.std(\"year\")\n",
    "\n",
    "    ## Get mask of grid cells exceeding 0.5 std threshold\n",
    "    threshold = slp_azores_mean + 0.5 * slp_azores_std\n",
    "    exceeds_thresh = slp_azores_norm > threshold\n",
    "\n",
    "    ## Sum area of lon/lat cells exceeding threshold\n",
    "    ## convert from True/False to 1.0/0.0 for integration\n",
    "    AHA = spatial_int(exceeds_thresh.astype(float))\n",
    "\n",
    "    ## convert from m^2 to km^2\n",
    "    m_per_km = 1000\n",
    "    m2_per_km2 = m_per_km**2\n",
    "    AHA *= 1 / m2_per_km2\n",
    "\n",
    "    return AHA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f65b45-0ba6-45b9-9da7-9f5d61e13477",
   "metadata": {},
   "source": [
    "### Data loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76322684-d192-4dae-901c-4e3fbcd78964",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def load_lme_member(forcing_type, member_id, fp_in):\n",
    "    \"\"\"\n",
    "    Function loads data from single member of CESM last-millenium ensemble (LME).\n",
    "    Args:\n",
    "    - 'member_id' is integer in [1,13] specifying which ensemble member to load\n",
    "    - 'forcing_type' is one of {\"all\",\"volcanic\",\"GHG\",\"orbital\"}\n",
    "    \"\"\"\n",
    "\n",
    "    ## get prefix\n",
    "    prefix = \"b.e11.BLMTRC5CN.f19_g16\"\n",
    "    if forcing_type == \"all\":\n",
    "        pass\n",
    "\n",
    "    elif forcing_type == \"GHG\":\n",
    "        prefix = f\"{prefix}.GHG\"\n",
    "\n",
    "    elif forcing_type == \"volcanic\":\n",
    "        prefix = f\"{prefix}.VOLC_GRA\"\n",
    "\n",
    "    elif forcing_type == \"orbital\":\n",
    "        prefix = f\"{prefix}.ORBITAL\"\n",
    "\n",
    "    else:\n",
    "        print(\"Error: not a valid forcing type\")\n",
    "        return\n",
    "\n",
    "    ## add rest of prefix\n",
    "    prefix = f\"{prefix}.{member_id:03d}.cam.h0.PSL\"\n",
    "\n",
    "    ## Get names of two files for each ensemble member\n",
    "    fp_and_prefix = os.path.join(fp_in, prefix)\n",
    "    fp0 = f\"{fp_and_prefix}.085001-184912.nc\"\n",
    "    fp1 = f\"{fp_and_prefix}.185001-200512.nc\"\n",
    "\n",
    "    ## Load data\n",
    "    data = xr.open_mfdataset([fp0, fp1], chunks={\"time\": 2000})[\"PSL\"]\n",
    "\n",
    "    ## switch longitude range from [0,360) to (-180,180]\n",
    "    data = switch_longitude_range(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_noaa(fp_slp_noaa):\n",
    "    \"\"\"NOAA CIRES data and update coordinates\"\"\"\n",
    "\n",
    "    ## open raw data and select PSL variable\n",
    "    data = xr.open_dataset(fp_slp_noaa)[\"prmsl\"]\n",
    "\n",
    "    ## switch longitude range from [0,360) to (-180,180]\n",
    "    data = switch_longitude_range(data)\n",
    "\n",
    "    ## reverse latitude direction from [90,-90] to [-90,90]\n",
    "    data = reverse_latitude(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_era(fp_slp_era):\n",
    "\n",
    "    ## open raw data and select PSL file\n",
    "    data = xr.open_dataarray(fp_slp_era)\n",
    "\n",
    "    ## rename coordinates from \"latitude\" and \"longitude\"\n",
    "    ## to \"lat\" and \"lon\"\n",
    "    data = data.rename({\"latitude\": \"lat\", \"longitude\": \"lon\"})\n",
    "\n",
    "    ## switch longitude range from [0,360) to (-180,180]\n",
    "    data = switch_longitude_range(data)\n",
    "\n",
    "    ## reverse latitude direction from [90,-90] to [-90,90]\n",
    "    data = reverse_latitude(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "## For each dataset, get DJF average and trim to North Atlantic\n",
    "def trim(data):\n",
    "    \"\"\"function to trim a data in time and space, and save to file.\n",
    "    Two datasets are returned:\n",
    "    - data_trim (trimmed to north atlantic)\n",
    "    - data_global_avg (global averaged of data)\"\"\"\n",
    "\n",
    "    ## trim in time, then load to memory\n",
    "    data_djf = djf_avg(data).compute()\n",
    "\n",
    "    ## trim in space\n",
    "    data_trim = trim_to_north_atlantic(data_djf)\n",
    "\n",
    "    ## get global average\n",
    "    data_global_avg = spatial_avg(data_djf)\n",
    "\n",
    "    ## combine into single dataset\n",
    "    data_prepped = xr.merge(\n",
    "        [data_trim.rename(\"slp\"), data_global_avg.rename(\"slp_global_avg\")]\n",
    "    )\n",
    "\n",
    "    return data_prepped\n",
    "\n",
    "\n",
    "def trim_to_north_atlantic(data):\n",
    "    \"\"\"convenience function to trim data north atlantic domain\"\"\"\n",
    "\n",
    "    return data.sel(lon=slice(-70, 15), lat=slice(0, 70))\n",
    "\n",
    "\n",
    "def trim_to_azores(data):\n",
    "    \"\"\"convenience function to trim data to Azores lon/lat range.\"\"\"\n",
    "\n",
    "    return data.sel(lon=slice(-60, 10), lat=slice(10, 52))\n",
    "\n",
    "\n",
    "def load_prepped_data(data_loader_fn, prep_fn, fp_out):\n",
    "    \"\"\"Function applies 'prep_fn' to data returned by 'data_loader_fn',\n",
    "    and saves result to 'fp_out'.\n",
    "    Args:\n",
    "        - data_loader_fn: function to load raw data\n",
    "        - prep_fn: function to pre-process raw data\n",
    "        - fp_out: filepath to save pre-processed data\n",
    "    \"\"\"\n",
    "\n",
    "    ## check if file exists\n",
    "    if os.path.isfile(fp_out):\n",
    "\n",
    "        ## Load pre-trimmed file\n",
    "        data_prepped = xr.open_dataset(fp_out).compute()\n",
    "\n",
    "    else:\n",
    "\n",
    "        ## Load data, trim it, and save to file\n",
    "        data = data_loader_fn()\n",
    "        data_prepped = prep_fn(data)\n",
    "        data_prepped.to_netcdf(fp_out)\n",
    "\n",
    "    return data_prepped\n",
    "\n",
    "\n",
    "def get_trimmed_data_lme(forcing_type, member_ids, fp_in, save_fp):\n",
    "    \"\"\"Process multiple ensemble members\"\"\"\n",
    "\n",
    "    ## Loop through each ensemble member\n",
    "    data_trimmed = []\n",
    "    for member_id in tqdm(member_ids):\n",
    "\n",
    "        ## get filepath for saving data\n",
    "        fp_out = pathlib.Path(save_fp, \"LME\", f\"LME_{forcing_type}_{member_id:03d}.nc\")\n",
    "\n",
    "        ## function to load the given ensemble member\n",
    "        data_loader_fn = lambda: load_lme_member(forcing_type, member_id, fp_in=fp_in)\n",
    "\n",
    "        ## load trimmed data\n",
    "        data_trimmed_i = load_prepped_data(data_loader_fn, trim, fp_out)\n",
    "\n",
    "        ## append result\n",
    "        data_trimmed.append(data_trimmed_i)\n",
    "\n",
    "    ## merge data from each ensemble member to single dataset\n",
    "    ensemble_member_dim = pd.Index(member_ids, name=\"ensemble_member\")\n",
    "    data_trimmed = xr.concat(data_trimmed, dim=ensemble_member_dim)\n",
    "\n",
    "    return data_trimmed\n",
    "\n",
    "\n",
    "def get_trimmed_data(data_loader_fn, fp_out):\n",
    "    return load_prepped_data(data_loader_fn=data_loader_fn, fp_out=fp_out, prep_fn=trim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6d2cae-bd42-4433-9666-2c9a86b8141d",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ca82c0-cf85-4027-8651-883f1db95a02",
   "metadata": {},
   "source": [
    "### Set filepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231445d6-5914-4840-9459-6ffd07528768",
   "metadata": {},
   "source": [
    "#### Save filepath "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4475cf-b1a0-4c37-806e-68dad30af351",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Where should we save intermediate results?\n",
    "save_fp = pathlib.Path(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c438465-fdeb-4c90-8c2a-35028e2fa167",
   "metadata": {},
   "source": [
    "#### Reanalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42c1899-6471-46e8-9e15-4f9a0d2fed1a",
   "metadata": {},
   "source": [
    "Note: I've downloaded this data locally (and saved it to the folder ```data/cmip5_reanalysis_raw```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed4bdce-64c8-474e-b4e3-a09e1315d77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_noaa_u10 = pathlib.Path(\"data/cmip5_reanalysis_raw/uwnd.10m.mon.mean.nc\")\n",
    "fp_noaa_v10 = pathlib.Path(\"data/cmip5_reanalysis_raw/vwnd.10m.mon.mean.nc\")\n",
    "fp_noaa_slp = pathlib.Path(\"data/cmip5_reanalysis_raw/prmsl.mon.mean.nc\")\n",
    "fp_noaa_precip = pathlib.Path(\"data/cmip5_reanalysis_raw/prate.mon.mean.nc\")\n",
    "fp_era_slp = pathlib.Path(\"data/cmip5_reanalysis_raw/msl.mon.mean.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434767cc-89ff-4116-a778-85db9967de28",
   "metadata": {},
   "source": [
    "The data was downloaded from the following locations on the CMIP5 server:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fc95e8-9fe9-416f-b480-c14c6f4b5de9",
   "metadata": {},
   "source": [
    "Path to the reanalysis data on the CMIP5 server:\n",
    "```python\n",
    "SERVER_FP = pathlib.Path(\"/Volumes/data\")\n",
    "CMIP5_REANALYSIS_FP = pathlib.Path(SERVER_FP, \"reanalysis\")\n",
    "```\n",
    "\n",
    "Paths to the NOAA 20th century reanalysis data products:\n",
    "```python\n",
    "NOAA_FP = pathlib.Path(CMIP5_REANALYSIS_FP, \"noaa.cires.20crv2c\")\n",
    "fp_noaa_u10 = pathlib.Path(NOAA_FP, \"gaussian/uwnd.10m/monthly/uwnd.10m.mon.mean.nc\")\n",
    "fp_noaa_v10 = pathlib.Path(NOAA_FP, \"gaussian/vwnd.10m/monthly/vwnd.10m.mon.mean.nc\")\n",
    "fp_noaa_slp = pathlib.Path(NOAA_FP, \"monolevel/prmsl/monthly/prmsl.mon.mean.nc\")\n",
    "fp_noaa_precip = pathlib.Path(NOAA_FP, \"gaussian/prate/monthly/prate.mon.mean.nc\")\n",
    "```\n",
    "\n",
    "Path to the ERA 20th century renalysis:\n",
    "```python\n",
    "fp_era_slp = pathlib.Path(CMIP5_REANALYSIS_FP, \"era.20c/sfc/msl/moda/msl.mon.mean.nc\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddc1c82-0d37-4199-86e9-dc59ffbc4525",
   "metadata": {},
   "source": [
    "#### LME\n",
    "Note: accesssing this data requires access to the shared data server called \"clidex\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9f6dbc-4807-4024-806f-4e9244b88183",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_lme = pathlib.Path(\"/vortexfs1/share/clidex/data/model/CESM/LME/atm/psl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5accfb32-4ce4-4524-bc90-ddc768a83b24",
   "metadata": {},
   "source": [
    "### LME filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057f2000-33d3-4219-bc23-636c8e4be814",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647d2bcb-18bf-4e27-8d0e-34e6f60b27f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reanalysis\n",
    "slp_noaa = get_trimmed_data(\n",
    "    data_loader_fn=lambda: load_noaa(fp_noaa_slp),\n",
    "    fp_out=pathlib.Path(save_fp, \"slp_noaa.nc\"),\n",
    ")\n",
    "slp_era = get_trimmed_data(\n",
    "    data_loader_fn=lambda: load_era(fp_era_slp),\n",
    "    fp_out=pathlib.Path(save_fp, \"slp_era.nc\"),\n",
    ")\n",
    "\n",
    "## LME\n",
    "kwargs = dict(save_fp=save_fp, fp_in=fp_lme)\n",
    "slp_lme = get_trimmed_data_lme(\n",
    "    forcing_type=\"all\", member_ids=np.arange(1, 14), **kwargs\n",
    ")\n",
    "slp_lme_volc = get_trimmed_data_lme(\n",
    "    forcing_type=\"volcanic\", member_ids=np.arange(1, 6), **kwargs\n",
    ")\n",
    "slp_lme_GHG = get_trimmed_data_lme(\n",
    "    forcing_type=\"GHG\", member_ids=np.arange(1, 4), **kwargs\n",
    ")\n",
    "slp_lme_orb = get_trimmed_data_lme(\n",
    "    forcing_type=\"orbital\", member_ids=np.arange(1, 4), **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0f7e10-e422-461d-843a-5de9b744a1da",
   "metadata": {},
   "source": [
    "### Plot SLP over time in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50a8048-7657-48f0-94e5-ef4ed9972c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## specify which dataset to use\n",
    "data = slp_noaa\n",
    "\n",
    "## Compute SLP averaged over Azores region\n",
    "slp_azores = spatial_avg(trim_to_azores(data[\"slp\"]))\n",
    "slp_global = data[\"slp_global_avg\"]\n",
    "\n",
    "## get linear trend for global\n",
    "global_trend = get_trend(slp_global)\n",
    "\n",
    "## Plot\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "ax.plot(data.year, slp_azores, label=\"Azores\")\n",
    "ax.plot(data.year, slp_global, label=\"Global\")\n",
    "ax.plot(data.year, global_trend, label=\"Global trend\", c=\"k\", ls=\"--\")\n",
    "ax.legend(prop=dict(size=8))\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"SLP (hPa)\")\n",
    "ax.set_yticks(ax.get_yticks(), labels=np.round(ax.get_yticks() / 100, 0).astype(int))\n",
    "plt.show()\n",
    "\n",
    "## Plot before/after normalizing\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "ax.plot(\n",
    "    slp_noaa.year,\n",
    "    (slp_azores - slp_global + slp_global.mean()),\n",
    "    label=\"remove global mean\",\n",
    ")\n",
    "ax.plot(\n",
    "    data.year,\n",
    "    (slp_azores - global_trend + global_trend.mean()),\n",
    "    label=\"remove trend\",\n",
    ")\n",
    "ax.legend(prop=dict(size=8))\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"SLP (hPa)\")\n",
    "ax.set_yticks(ax.get_yticks(), labels=np.round(ax.get_yticks() / 100, 0).astype(int))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b170b91f-59fc-441a-a794-5022d95e6c6c",
   "metadata": {},
   "source": [
    "## Compute AHA metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25c795d-70ba-42ba-badf-b3b7c922db0f",
   "metadata": {},
   "source": [
    "From Cresswell-Clay et al. (2022): \"The AHA was defined as the area (km2) over the North Atlantic and Western Europe that had mean winter (December–January–February) SLP greater than 0.5 s.d. from the mean of the spatio-temporal winter SLP distribution (Fig. 1b). The region considered when calculating the AHA is bounded by the 60° W and 10° E meridians as well as the 10° N and 52° N latitudes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02836efb-682a-484e-81d2-949512b41348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_AHA_wrapper(slp_data, save_fp):\n",
    "    \"\"\"Load AHA from given save_fp. If file doesn't exist,\n",
    "    compute AHA based on given SLP data and save result\"\"\"\n",
    "\n",
    "    try:\n",
    "        AHA = xr.open_dataarray(save_fp)\n",
    "\n",
    "    except:\n",
    "        AHA = compute_AHA(\n",
    "            slp_data[\"slp\"], slp_data[\"slp_global_avg\"], norm_type=\"detrend\"\n",
    "        )\n",
    "        AHA.to_netcdf(save_fp)\n",
    "\n",
    "    return AHA\n",
    "\n",
    "\n",
    "## specify directory for saving AHA results\n",
    "AHA_save_fp = pathlib.Path(save_fp, \"AHA\")\n",
    "\n",
    "## compute AHA metric for each dataset\n",
    "AHA_noaa = compute_AHA_wrapper(slp_noaa, AHA_save_fp / \"AHA_noaa.nc\")\n",
    "AHA_era = compute_AHA_wrapper(slp_era, AHA_save_fp / \"AHA_era.nc\")\n",
    "AHA_lme = compute_AHA_wrapper(slp_lme, AHA_save_fp / \"AHA_lme_full.nc\")\n",
    "AHA_lme_orbital = compute_AHA_wrapper(slp_lme_orb, AHA_save_fp / \"AHA_lme_orb.nc\")\n",
    "AHA_lme_volc = compute_AHA_wrapper(slp_lme_volc, AHA_save_fp / \"AHA_lme_volc.nc\")\n",
    "AHA_lme_ghg = compute_AHA_wrapper(slp_lme_GHG, AHA_save_fp / \"AHA_lme_GHG.nc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
